# -*- coding: utf-8 -*-
"""
This example shows how to build an agent with RAG (backup by LlamaIndex)
"""

from typing import Optional
from loguru import logger

from agentscope.prompt import PromptType
from agentscope.agents.agent import AgentBase
from agentscope.prompt import PromptEngine
from agentscope.message import Msg
from agentscope.models import load_model_by_config_name
from agentscope.rag.llama_index_rag import LlamaIndexRAG
from agentscope.rag.langchain_rag import LangChainRAG


class RAGAgentBase(AgentBase):
    """
    Base class for RAG agents, child classes include the
    RAG agents built with LlamaIndex and LangChain in this file
    """

    def __init__(
        self,
        name: str,
        sys_prompt: str,
        model_config_name: str,
        emb_model_config_name: str,
        memory_config: Optional[dict] = None,
        prompt_type: Optional[PromptType] = PromptType.LIST,
        config: Optional[dict] = None,
    ) -> None:
        """
        Initialize the RAG base agent
        Args:
            name (str): the name for the agent
            sys_prompt (str): system prompt for the RAG agent
            model_config_name (str): language model for the agent
            emb_model_config_name (str): embedding model for the agent
            memory_config (dict): memory configuration
            prompt_type (PromptType): prompt type, list or str
            config (dict): additional config for RAG and agent
                Current support adjustable parameters includes:
                "data_path":
                    path to data directory where data is stored,
                "chunk_size":
                    maximum chunk size for preprocessed documents,
                    default is agentscope.rag.rag.DEFAULT_CHUNK_SIZE
                "chunk_overlap":
                    overlap between preprocessed chunks, default is
                    agentscope.rag.rag.DEFAULT_CHUNK_OVERLAP
                "similarity_top_k":
                    number of chunks in each retrieval, default
                    is agentscope.rag.rag.DEFAULT_TOP_K
                "log_retrieval" (bool):
                    whether to user agent.speak() to print the
                    retrieved documents, default is False.
                "recent_n_mem":
                    how many messages in memory is used as query
                    for retrieval if memory is used, default is 1
        """
        super().__init__(
            name=name,
            sys_prompt=sys_prompt,
            model_config_name=model_config_name,
            use_memory=True,
            memory_config=memory_config,
        )
        # init prompt engine
        self.engine = PromptEngine(self.model, prompt_type=prompt_type)
        self.emb_model = load_model_by_config_name(emb_model_config_name)

        # init rag as None
        # MUST USE LlamaIndexAgent OR LangChainAgent
        self.rag = None
        self.config = config or {}
        if "log_retrieval" not in self.config:
            self.config["log_retrieval"] = True

    def reply(
        self,
        x: dict = None,
    ) -> dict:
        """
        Reply function of the RAG agent.
        Processes the input data,
        1) use the input data to retrieve with RAG function;
        2) generates a prompt using the current memory and system
        prompt;
        3) invokes the language model to produce a response. The
        response is then formatted and added to the dialogue memory.

        Args:
            x (`dict`, defaults to `None`):
                A dictionary representing the user's input to the agent. This
                input is added to the memory if provided. Defaults to
                None.
        Returns:
            A dictionary representing the message generated by the agent in
            response to the user's input.
        """
        retrieved_docs_to_string = ""
        # record the input if needed
        if self.memory:
            self.memory.add(x)
            # in case no input is provided (e.g., in msghub),
            # use the memory as query
            history = self.engine.join(
                self.memory.get_memory(
                    recent_n=self.config.get("recent_n_mem", 1),
                ),
            )
            query = (
                "/n".join(
                    [msg["content"] for msg in history],
                )
                if isinstance(history, list)
                else str(history)
            )
        elif x is not None:
            query = x["content"]
        else:
            query = ""

        if len(query) > 0:
            # when content has information, do retrieval
            retrieved_docs = self.rag.retrieve(query, to_list_strs=True)
            for content in retrieved_docs:
                retrieved_docs_to_string += "\n>>>> " + content

            if self.config["log_retrieval"]:
                self.speak("[retrieved]:" + retrieved_docs_to_string)

        # prepare prompt
        prompt = self.engine.join(
            {
                "role": "system",
                "content": self.sys_prompt,
            },
            # {"role": "system", "content": retrieved_docs_to_string},
            self.memory.get_memory(),
            "Context: " + retrieved_docs_to_string,
        )

        # call llm and generate response
        response = self.model(prompt).text
        msg = Msg(self.name, response)

        # Print/speak the message in this agent's voice
        self.speak(msg)

        if self.memory:
            # Record the message in memory
            self.memory.add(msg)

        return msg


class LlamaIndexAgent(RAGAgentBase):
    """
    A LlamaIndex agent build on LlamaIndex.
    """

    def __init__(
        self,
        name: str,
        sys_prompt: str,
        model_config_name: str,
        emb_model_config_name: str = None,
        memory_config: Optional[dict] = None,
        prompt_type: Optional[PromptType] = PromptType.LIST,
        config: Optional[dict] = None,
    ) -> None:
        """
        Initialize the RAG LlamaIndexAgent
        Args:
            name (str): the name for the agent
            sys_prompt (str): system prompt for the RAG agent
            model_config_name (str): language model for the agent
            emb_model_config_name (str): embedding model for the agent
            memory_config (dict): memory configuration
            prompt_type (PromptType): prompt type, list or str
            config (dict): additional config for RAG and agent
                Current support adjustable parameters includes:
                "data_path":
                    path to data directory where data is stored,
                "chunk_size":
                    maximum chunk size for preprocessed documents,
                    default is agentscope.rag.rag.DEFAULT_CHUNK_SIZE
                "chunk_overlap":
                    overlap between preprocessed chunks, default is
                    agentscope.rag.rag.DEFAULT_CHUNK_OVERLAP
                "similarity_top_k":
                    number of chunks in each retrieval, default
                    is agentscope.rag.rag.DEFAULT_TOP_K
                "log_retrieval" (bool):
                    whether to user agent.speak() to print the
                    retrieved documents, default is False.
                "recent_n_mem":
                    how many messages in memory is used as query
                    for retrieval if memory is used, default is 1
        """
        super().__init__(
            name=name,
            sys_prompt=sys_prompt,
            model_config_name=model_config_name,
            emb_model_config_name=emb_model_config_name,
            memory_config=memory_config,
            prompt_type=prompt_type,
            config=config,
        )
        try:
            from llama_index.core import SimpleDirectoryReader
        except ImportError as exc:
            raise ImportError(
                " LlamaIndexAgent requires llama-index to be install."
                "Please run `pip install llama-index`",
            ) from exc
        # init rag related attributes
        self.rag = LlamaIndexRAG(
            model=self.model,
            emb_model=self.emb_model,
            config=config,
        )
        # load the document to memory
        # Feed the AgentScope tutorial documents, so that
        # the agent can answer questions related to AgentScope!
        if "data_path" not in self.config:
            self.config[" data_path"] = "./data"
            logger.warning(
                "No data_path provided in RAG agent config,"
                "use default path `./data`",
            )
        docs = self.rag.load_data(
            loader=SimpleDirectoryReader(self.config["data_path"]),
        )
        self.rag.store_and_index(docs)


class LangChainRAGAgent(RAGAgentBase):
    """
    A LlamaIndex agent build on LlamaIndex.
    """

    def __init__(
        self,
        name: str,
        sys_prompt: str,
        model_config_name: str,
        emb_model_config_name: str,
        memory_config: Optional[dict] = None,
        prompt_type: Optional[PromptType] = PromptType.LIST,
        config: Optional[dict] = None,
    ) -> None:
        """
        Initialize the RAG LangChainRAGAgent
        Args:
            name (str): the name for the agent
            sys_prompt (str): system prompt for the RAG agent
            model_config_name (str): language model for the agent
            emb_model_config_name (str): embedding model for the agent
            memory_config (dict): memory configuration
            prompt_type (PromptType): prompt type, list or str
            config (dict): additional config for RAG and agent
                Current support adjustable parameters includes:
                "data_path":
                    path to data directory where data is stored,
                "chunk_size":
                    maximum chunk size for preprocessed documents,
                    default is agentscope.rag.rag.DEFAULT_CHUNK_SIZE
                "chunk_overlap":
                    overlap between preprocessed chunks, default is
                    agentscope.rag.rag.DEFAULT_CHUNK_OVERLAP
                "similarity_top_k":
                    number of chunks in each retrieval, default
                    is agentscope.rag.rag.DEFAULT_TOP_K
                "log_retrieval" (bool):
                    whether to user agent.speak() to print the
                    retrieved documents, default is False.
                "recent_n_mem":
                    how many messages in memory is used as query
                    for retrieval if memory is used, default is 1
        """
        super().__init__(
            name=name,
            sys_prompt=sys_prompt,
            model_config_name=model_config_name,
            emb_model_config_name=emb_model_config_name,
            memory_config=memory_config,
            prompt_type=prompt_type,
            config=config,
        )
        try:
            from langchain_community.document_loaders import DirectoryLoader
        except ImportError as exc:
            raise ImportError(
                "LangChainRAGAgent requires LangChain related packages "
                "installed. Please run `pip install langchain "
                "unstructured[all-docs] langchain-text-splitters`",
            ) from exc

        # init rag related attributes
        self.rag = LangChainRAG(
            model=self.model,
            emb_model=self.emb_model,
            config=config,
        )
        # load the document to memory
        # Feed the AgentScope tutorial documents, so that
        # the agent can answer questions related to AgentScope!
        if "data_path" not in self.config:
            self.config[" data_path"] = "./data"
            logger.warning(
                "No data_path provided in RAG agent config,"
                "use default path `./data`",
            )
        docs = self.rag.load_data(
            loader=DirectoryLoader(self.config["data_path"]),
        )
        self.rag.store_and_index(docs)
