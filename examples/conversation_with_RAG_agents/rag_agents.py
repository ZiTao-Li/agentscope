# -*- coding: utf-8 -*-
"""
This example shows how to build an agent with RAG
with LlamaIndex.

Notice, this is a Beta version of RAG agent.
"""

from abc import ABC, abstractmethod
from typing import Optional, Any
from loguru import logger

from rag import RAGBase, LlamaIndexRAG

from agentscope.agents.agent import AgentBase
from agentscope.message import Msg
from agentscope.models import load_model_by_config_name


class RAGAgentBase(AgentBase, ABC):
    """
    Base class for RAG agents
    """

    def __init__(
        self,
        name: str,
        sys_prompt: str,
        model_config_name: str,
        emb_model_config_name: str,
        memory_config: Optional[dict] = None,
        rag_config: Optional[dict] = None,
    ) -> None:
        """
        Initialize the RAG base agent
        Args:
            name (str):
                the name for the agent.
            sys_prompt (str):
                system prompt for the RAG agent.
            model_config_name (str):
                language model for the agent.
            emb_model_config_name (str):
                embedding model for the agent.
            memory_config (dict):
                memory configuration.
            rag_config (dict):
                config for RAG. It contains most of the
                important parameters for RAG modules. If not provided,
                the default setting will be used.
                Examples can refer to children classes.
        """
        super().__init__(
            name=name,
            sys_prompt=sys_prompt,
            model_config_name=model_config_name,
            use_memory=True,
            memory_config=memory_config,
        )
        # setup embedding model used in RAG
        self.emb_model = load_model_by_config_name(emb_model_config_name)

        # setup RAG configurations
        self.rag_config = rag_config or {}

        # use LlamaIndexAgent OR LangChainAgent
        self.rag = self.init_rag()

    @abstractmethod
    def init_rag(self) -> RAGBase:
        """initialize RAG with configuration"""

    def reply(
        self,
        x: dict = None,
    ) -> dict:
        """
        Reply function of the RAG agent.
        Processes the input data,
        1) use the input data to retrieve with RAG function;
        2) generates a prompt using the current memory and system
        prompt;
        3) invokes the language model to produce a response. The
        response is then formatted and added to the dialogue memory.

        Args:
            x (`dict`, defaults to `None`):
                A dictionary representing the user's input to the agent. This
                input is added to the memory if provided. Defaults to
                None.
        Returns:
            A dictionary representing the message generated by the agent in
            response to the user's input.
        """
        retrieved_docs_to_string = ""
        # record the input if needed
        if self.memory:
            self.memory.add(x)
            # in case no input is provided (e.g., in msghub),
            # use the memory as query
            history = self.memory.get_memory(
                recent_n=self.rag_config.get("recent_n_mem", 1),
            )
            query = (
                "/n".join(
                    [msg["content"] for msg in history],
                )
                if isinstance(history, list)
                else str(history)
            )
        elif x is not None:
            query = x["content"]
        else:
            query = ""

        if len(query) > 0:
            # when content has information, do retrieval
            retrieved_docs = self.rag.retrieve(query, to_list_strs=True)
            for content in retrieved_docs:
                retrieved_docs_to_string += "\n>>>> " + content

            if self.rag_config["log_retrieval"]:
                self.speak("[retrieved]:" + retrieved_docs_to_string)

        # prepare prompt
        prompt = self.model.format(
            Msg(
                name="system",
                role="system",
                content=self.sys_prompt,
            ),
            # {"role": "system", "content": retrieved_docs_to_string},
            self.memory.get_memory(
                recent_n=self.rag_config.get("recent_n_mem", 1),
            ),
            Msg(
                name="user",
                role="user",
                content="Context: " + retrieved_docs_to_string,
            ),
        )

        # call llm and generate response
        response = self.model(prompt).text
        msg = Msg(self.name, response)

        # Print/speak the message in this agent's voice
        self.speak(msg)

        if self.memory:
            # Record the message in memory
            self.memory.add(msg)

        return msg


class LlamaIndexAgent(RAGAgentBase):
    """
    A LlamaIndex agent build on LlamaIndex.
    """

    def __init__(
        self,
        name: str,
        sys_prompt: str,
        model_config_name: str,
        emb_model_config_name: str = None,
        memory_config: Optional[dict] = None,
        rag_config: Optional[dict] = None,
        **kwargs: Any,
    ) -> None:
        """
        Initialize the RAG LlamaIndexAgent
        Args:
            name (str):
                the name for the agent
            sys_prompt (str):
                system prompt for the RAG agent
            model_config_name (str):
                language model for the agent
            emb_model_config_name (str):
                embedding model for the agent
            memory_config (dict):
                memory configuration
            rag_config (dict):
                config for RAG. It contains the parameters for
                RAG modules functions:
                rag.load_data(...) and rag.store_and_index(docs, ...)
                 If not provided, the default setting will be used.
                An example of the config for retrieving code files
                is as following:

                "rag_config":{
                    "index_configs": [
                      {
                        "load_data": {
                          "loader": {
                            "create_object": true,
                            "module": "llama_index.core",
                            "class": "SimpleDirectoryReader",
                            "init_args": {
                              "input_dir": "path/to/data",
                              "recursive": true
                              ...
                            }
                          }
                        },
                        "store_and_index": {
                          "transformations": [
                            {
                              "create_object": true,
                              "module": "llama_index.core.node_parser",
                              "class": "CodeSplitter",
                              "init_args": {
                                "language": "python",
                                "chunk_lines": 100
                              }
                            }
                          ]
                        }
                      }
                    ],
                    "chunk_size": 2048,
                    "chunk_overlap": 40,
                    "similarity_top_k": 10,
                    "log_retrieval": true,
                    "recent_n_mem": 1
                  }
        """
        super().__init__(
            name=name,
            sys_prompt=sys_prompt,
            model_config_name=model_config_name,
            emb_model_config_name=emb_model_config_name,
            memory_config=memory_config,
            rag_config=rag_config,
        )
        self.description = kwargs.get("description", "")

    def init_rag(self) -> LlamaIndexRAG:
        # dynamic loading loader
        # initiate RAG related attributes
        rag = LlamaIndexRAG(
            model=self.model,
            emb_model=self.emb_model,
            rag_config=self.rag_config,
            index_config=self.rag_config.get("index_config"),
        )
        return rag

    def reply(
        self,
        x: dict = None,
    ) -> dict:
        """
        Reply function of the RAG agent.
        Processes the input data,
        1) use the input data to retrieve with RAG function;
        2) generates a prompt using the current memory and system
        prompt;
        3) invokes the language model to produce a response. The
        response is then formatted and added to the dialogue memory.

        Args:
            x (`dict`, defaults to `None`):
                A dictionary representing the user's input to the agent. This
                input is added to the memory if provided. Defaults to
                None.
        Returns:
            A dictionary representing the message generated by the agent in
            response to the user's input.
        """
        retrieved_docs_to_string = ""
        # record the input if needed
        if self.memory:
            self.memory.add(x)
            # in case no input is provided (e.g., in msghub),
            # use the memory as query
            history = self.memory.get_memory(
                recent_n=self.rag_config.get("recent_n_mem", 1),
            )
            query = (
                "/n".join(
                    [msg["content"] for msg in history],
                )
                if isinstance(history, list)
                else str(history)
            )
        elif x is not None:
            query = x["content"]
        else:
            query = ""

        if len(query) > 0:
            # when content has information, do retrieval
            retrieved_docs = self.rag.retrieve(query)
            scores = []
            for content in retrieved_docs:
                scores.append(content.score)
                retrieved_docs_to_string += (
                    "\n>>>> score:"
                    + str(content.score)
                    + "\n>>>> content:"
                    + content.get_content()
                )

            if self.rag_config["log_retrieval"]:
                self.speak("[retrieved]:" + retrieved_docs_to_string)

            if max(scores) < 0.4:
                # if the max score is lower than 0.4, then we let LLM
                # to decide whether the retrieved content is relevant
                # to the user input.
                CHECKING_PROMPT = """
                Does the retrieved content is relevant to the query?
                Retrieved content: {}
                Query: {}
                Only answer YES or NO.
                """
                msg = Msg(
                    name="user",
                    role="user",
                    content=CHECKING_PROMPT.format(
                        retrieved_docs_to_string,
                        query,
                    ),
                )
                print(msg)
                checking = self.model([msg])
                logger.info(checking)
                checking = checking.text.lower()
                if "no" in checking:
                    retrieved_docs_to_string = "EMPTY"

        # prepare prompt
        prompt = self.model.format(
            Msg(
                name="system",
                role="system",
                content=self.sys_prompt,
            ),
            # {"role": "system", "content": retrieved_docs_to_string},
            self.memory.get_memory(
                recent_n=self.rag_config.get("recent_n_mem", 1),
            ),
            Msg(
                name="user",
                role="user",
                content="Context: " + retrieved_docs_to_string,
            ),
        )

        # call llm and generate response
        response = self.model(prompt).text
        msg = Msg(self.name, response)

        # Print/speak the message in this agent's voice
        self.speak(msg)

        if self.memory:
            # Record the message in memory
            self.memory.add(msg)

        return msg
